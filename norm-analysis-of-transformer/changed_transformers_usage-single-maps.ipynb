{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage of modified transformers library (modification have been done only for BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the changed library (only first once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. move into the modified library\n",
    "> $ cd transformers\n",
    "\n",
    "1. install using pip (I recommend editable mode (-e option).)\n",
    "> $ pip install -e .\n",
    "\n",
    "1. please install pytorch (https://pytorch.org/) \\\n",
    " e.g., \n",
    " > $ pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer, BartModel, BartTokenizer, BartConfig, T5Model, T5Tokenizer\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model/tokenizer\n",
    "model_name = \"t5-large\" #\"facebook/bart-large\" #\"bert-large-uncased-whole-word-masking\"\n",
    "#tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "#model = BertModel.from_pretrained(model_name).to(device)\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "# model = RobertaModel.from_pretrained(model_name).to(device)\n",
    "# tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "# model = BartModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.mask_token = \"<extra_id_0>\"\n",
    "model = T5Model.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Tokenize text and convert to ids. \n",
    "# input_ids = torch.tensor([tokenizer.encode(\"A 41 year old person age is [MASK] than a 42 year old person .\", add_special_tokens=True)]).to(device)\n",
    "input_ids = torch.tensor([tokenizer.encode(f\"A 41 year old person age is {tokenizer.mask_token} than a 42 year old person .\", add_special_tokens=True)]).to(device)\n",
    "#input_ids = torch.tensor([tokenizer.encode(\"The size of a feather is usually much <mask> than the size of a nail .\", add_special_tokens=True)]).to(device)\n",
    "tokenized_text = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to extract vector norms (i.e., ||f(x)||, ||αf(x)||, ||Σαf(x)||), please set the argument \"output_norms\" to True.\n",
    "with torch.no_grad():\n",
    "    #last_hidden_state, pooler_output, hidden_states, attentions, norms\n",
    "#     decoder_features, decoder_states, decoder_attns, encoder_last_state, encoder_states, encoder_attns, norms = model(input_ids=input_ids.repeat(2, 1), output_hidden_states=True, output_attentions=True, output_norms=True)    \n",
    "#     decoder_features, decoder_states, decoder_attns, decoder_self_norms, decoder_cross_norms, encoder_last_state, encoder_states, encoder_attns, encoder_norms = model(input_ids=input_ids, output_hidden_states=True, output_attentions=True, output_norms=True)  # BART\n",
    "    decoder_features, decoder_cache, decoder_states, decoder_attns, decoder_self_norms, decoder_cross_norms, encoder_last_state, encoder_states, encoder_attns, encoder_norms = model(input_ids=input_ids, decoder_input_ids=input_ids, output_hidden_states=True, output_attentions=True, output_norms=True)  # T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = encoder_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = model.config.num_attention_heads\n",
    "num_layers = model.config.num_hidden_layers\n",
    "#fig, axes = plt.subplots(num_layers, num_heads)\n",
    "#fig = plt.figure()\n",
    "#fig.suptitle(f\"BERT-Large-WWM Attention Norms\")\n",
    "fig = plt.figure() #plt.figure(figsize=(16, 12)) \n",
    "fig.set_figheight(120)\n",
    "fig.set_figwidth(160)\n",
    "#plt.rcParams[\"figure.figsize\"] = (15 * num_layers, 15 * num_heads)\n",
    "# num_layers = 6\n",
    "for layer in tqdm(range(num_layers)):\n",
    "    for head in range(num_heads):\n",
    "        plt.subplot(num_heads, num_layers, num_layers * head + layer + 1)\n",
    "        real_layer = layer\n",
    "#         real_layer = layer if layer < 3 else layer + 18\n",
    "#         if (layer == 0) or (layer == 23): #or ((layer in layer2head) and (head in layer2head[layer])):\n",
    "#         afx_norm = norms[real_layer][0][1]  # bart encoder-self\n",
    "        afx_norm = norms[real_layer][1]  # bart decoder-self, bart cross, t5 all\n",
    "        norm = afx_norm[0][head].cpu().numpy()\n",
    "        df = pd.DataFrame(norm,columns=tokenized_text,index=tokenized_text)\n",
    "        sns.heatmap(df,cmap=\"Reds\",square=True)\n",
    "        plt.gcf().subplots_adjust(bottom=0.2)\n",
    "        \n",
    "plt.savefig(f\"t5-encoder-self-attention-fixed.png\", bbox_inches=\"tight\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = decoder_cross_norms\n",
    "num_heads = model.config.num_attention_heads\n",
    "num_layers = model.config.num_hidden_layers\n",
    "#fig, axes = plt.subplots(num_layers, num_heads)\n",
    "#fig = plt.figure()\n",
    "#fig.suptitle(f\"BERT-Large-WWM Attention Norms\")\n",
    "fig = plt.figure() #plt.figure(figsize=(16, 12)) \n",
    "fig.set_figheight(120)\n",
    "fig.set_figwidth(160)\n",
    "#plt.rcParams[\"figure.figsize\"] = (15 * num_layers, 15 * num_heads)\n",
    "# num_layers = 6\n",
    "for layer in tqdm(range(num_layers)):\n",
    "    for head in range(num_heads):\n",
    "        plt.subplot(num_heads, num_layers, num_layers * head + layer + 1)\n",
    "        real_layer = layer\n",
    "#         real_layer = layer if layer < 3 else layer + 18\n",
    "#         if (layer == 0) or (layer == 23): #or ((layer in layer2head) and (head in layer2head[layer])):\n",
    "#         afx_norm = norms[real_layer][0][1]  # bart encoder-self\n",
    "        afx_norm = norms[real_layer][1]  # bart decoder-self, bart cross, t5 all\n",
    "        norm = afx_norm[0][head].cpu().numpy()\n",
    "        df = pd.DataFrame(norm,columns=tokenized_text,index=tokenized_text)\n",
    "        sns.heatmap(df,cmap=\"Reds\",square=True)\n",
    "        plt.gcf().subplots_adjust(bottom=0.2)\n",
    "        \n",
    "#     time.sleep(30)\n",
    "\n",
    "plt.savefig(f\"t5-cross-attention-fixed-2.png\", bbox_inches=\"tight\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = decoder_self_norms\n",
    "num_heads = model.config.num_attention_heads\n",
    "num_layers = model.config.num_hidden_layers\n",
    "#fig, axes = plt.subplots(num_layers, num_heads)\n",
    "#fig = plt.figure()\n",
    "#fig.suptitle(f\"BERT-Large-WWM Attention Norms\")\n",
    "fig = plt.figure() #plt.figure(figsize=(16, 12)) \n",
    "fig.set_figheight(120)\n",
    "fig.set_figwidth(160)\n",
    "#plt.rcParams[\"figure.figsize\"] = (15 * num_layers, 15 * num_heads)\n",
    "# num_layers = 6\n",
    "for layer in tqdm(range(num_layers)):\n",
    "    for head in range(num_heads):\n",
    "        plt.subplot(num_heads, num_layers, num_layers * head + layer + 1)\n",
    "        real_layer = layer\n",
    "#         real_layer = layer if layer < 3 else layer + 18\n",
    "#         if (layer == 0) or (layer == 23): #or ((layer in layer2head) and (head in layer2head[layer])):\n",
    "#         afx_norm = norms[real_layer][0][1]  # bart encoder-self\n",
    "        afx_norm = norms[real_layer][1]  # bart decoder-self, bart cross, t5 all\n",
    "        norm = afx_norm[0][head].cpu().numpy()\n",
    "        df = pd.DataFrame(norm,columns=tokenized_text,index=tokenized_text)\n",
    "        sns.heatmap(df,cmap=\"Reds\",square=True)\n",
    "        plt.gcf().subplots_adjust(bottom=0.2)\n",
    "        \n",
    "#     time.sleep(30)\n",
    "        \n",
    "plt.savefig(f\"t5-decoder-self-attention-fixed.png\", bbox_inches=\"tight\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END of oLMpics visualizations\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Code below is from the original kobayashi notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention weight visualization (Head-level visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure() \n",
    "attention = attentions[layer-1][0][head-1].cpu().numpy()\n",
    "df = pd.DataFrame(attention,columns=tokenized_text,index=tokenized_text)\n",
    "sns.heatmap(df,cmap=\"Reds\",square=True)\n",
    "plt.gcf().subplots_adjust(bottom=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm of the weighted vectors (||αf(x)||) visualization (Head-level visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2head = {2: [2, 3], 3: [12], 5: [2], 6: [6], 12: [3], 13: [4], 15: [8], 16: [0, 8], 17: [2, 5, 10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = model.config.num_attention_heads\n",
    "num_layers = model.config.num_hidden_layers\n",
    "#fig, axes = plt.subplots(num_layers, num_heads)\n",
    "#fig = plt.figure()\n",
    "#fig.suptitle(f\"BERT-Large-WWM Attention Norms\")\n",
    "fig = plt.figure() #plt.figure(figsize=(16, 12)) \n",
    "fig.set_figheight(120)\n",
    "fig.set_figwidth(160)\n",
    "#plt.rcParams[\"figure.figsize\"] = (15 * num_layers, 15 * num_heads)\n",
    "for layer in tqdm(range(num_layers)):\n",
    "    for head in range(num_heads):\n",
    "        plt.subplot(num_heads, num_layers, num_layers * head + layer + 1)\n",
    "        if (layer == 0) or (layer == 23): #or ((layer in layer2head) and (head in layer2head[layer])):\n",
    "            afx_norm = norms[layer][1]\n",
    "            norm = afx_norm[0][head].cpu().numpy()\n",
    "            df = pd.DataFrame(norm,columns=tokenized_text,index=tokenized_text)\n",
    "            sns.heatmap(df,cmap=\"Reds\",square=True)\n",
    "            plt.gcf().subplots_adjust(bottom=0.2)\n",
    "        \n",
    "plt.savefig(f\"{model_name}-attention-important_all.png\", bbox_inches=\"tight\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "head = 1\n",
    "plt.figure() \n",
    "afx_norm = norms[layer-1][0][1]  # BART\n",
    "norm = afx_norm[0][head-1].cpu().numpy()\n",
    "df = pd.DataFrame(norm,columns=tokenized_text,index=tokenized_text)\n",
    "sns.heatmap(df,cmap=\"Reds\",square=True)\n",
    "plt.gcf().subplots_adjust(bottom=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "head = 1\n",
    "plt.figure() \n",
    "afx_norm = norms[layer-1][1]\n",
    "norm = afx_norm[0][head-1].cpu().numpy()\n",
    "df = pd.DataFrame(norm,columns=tokenized_text,index=tokenized_text)\n",
    "sns.heatmap(df,cmap=\"Reds\",square=True)\n",
    "plt.gcf().subplots_adjust(bottom=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention weight visualization (Layer-level visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure() \n",
    "attention = attentions[layer-1][0].mean(0).cpu().numpy()\n",
    "df = pd.DataFrame(attention,columns=tokenized_text,index=tokenized_text)\n",
    "sns.heatmap(df,cmap=\"Reds\",square=True)\n",
    "plt.gcf().subplots_adjust(bottom=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm of the summed weighted vectors (||Σ αf(x)||) visualization (Layer-level visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure() \n",
    "summed_afx_norm = norms[layer-1][2]\n",
    "norm = summed_afx_norm[0].cpu().numpy()\n",
    "df = pd.DataFrame(norm,columns=tokenized_text,index=tokenized_text)\n",
    "sns.heatmap(df,cmap=\"Reds\",square=True)\n",
    "plt.gcf().subplots_adjust(bottom=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
