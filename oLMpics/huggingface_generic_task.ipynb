{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e163e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import transformers\n",
    "import wandb\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s: %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64aee282",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CustomArguments(transformers.TrainingArguments):\n",
    "    sample_train: int = 0\n",
    "    sample_eval: int = 0\n",
    "    num_choices: int = 0\n",
    "    model_name_or_path: str = \"asdf\"  # this is no longer a TrainingArgument attribute\n",
    "        \n",
    "    # python dataclasses cannot have positional attributes in subclass,\n",
    "    # so give all attributes defaults and then make sure they are changed\n",
    "    def __post_init__(self):\n",
    "        if not (self.sample_train * self.sample_eval * self.num_choices) or \\\n",
    "               self.model_name_or_path == \"asdf\":  # make sure none are still default value\n",
    "            raise TypeError(\"__init__ missing required argument(s)\")\n",
    "\n",
    "def get_args():\n",
    "    \"\"\" Set hyperparameters \"\"\"\n",
    "    args = CustomArguments(\n",
    "        output_dir=\"checkpoint\",\n",
    "        model_name_or_path=\"roberta-base\",\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=False,  # Zero shot\n",
    "        do_eval=True,\n",
    "        per_device_eval_batch_size=8,\n",
    "        learning_rate=1e-5,  # Should not matter because not training\n",
    "        weight_decay=0.1,\n",
    "        save_total_limit=2,\n",
    "        seed=123,\n",
    "        sample_train=200,\n",
    "        sample_eval=-1,\n",
    "        num_choices=2,\n",
    "    )\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b031c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_path, sample, num_choices):\n",
    "    data_file = open(file_path, \"r\")\n",
    "    logger.info(\"Reading QA instances from jsonl dataset at: %s\", file_path)\n",
    "    item_jsons = []\n",
    "    item_ids = []\n",
    "    questions = []\n",
    "    choice_lists = []\n",
    "    answer_ids = []\n",
    "    for line in data_file:\n",
    "        item_jsons.append(json.loads(line.strip()))\n",
    "\n",
    "    if sample != -1:\n",
    "        item_jsons = random.sample(item_jsons, sample)\n",
    "        logger.info(\"Sampling %d examples\", sample)\n",
    "\n",
    "    for item_json in tqdm(item_jsons,total=len(item_jsons)):\n",
    "        item_id = item_json[\"id\"]\n",
    "\n",
    "        question_text = item_json[\"question\"][\"stem\"]\n",
    "\n",
    "        choice_label_to_id = {}\n",
    "        choice_text_list = []\n",
    "        choice_context_list = []\n",
    "        choice_label_list = []\n",
    "        choice_annotations_list = []\n",
    "\n",
    "        any_correct = False\n",
    "        choice_id_correction = 0\n",
    "\n",
    "        for choice_id, choice_item in enumerate(item_json[\"question\"][\"choices\"]):\n",
    "            choice_label = choice_item[\"label\"]\n",
    "            choice_label_to_id[choice_label] = choice_id - choice_id_correction\n",
    "            choice_text = choice_item[\"text\"]\n",
    "\n",
    "            choice_text_list.append(choice_text)\n",
    "            choice_label_list.append(choice_label)\n",
    "\n",
    "            if item_json.get('answerKey') == choice_label:\n",
    "                if any_correct:\n",
    "                    raise ValueError(\"More than one correct answer found for {item_json}!\")\n",
    "                any_correct = True\n",
    "\n",
    "\n",
    "        if not any_correct and 'answerKey' in item_json:\n",
    "            raise ValueError(\"No correct answer found for {item_json}!\")\n",
    "\n",
    "\n",
    "        answer_id = choice_label_to_id.get(item_json.get(\"answerKey\"))\n",
    "        # Pad choices with empty strings if not right number\n",
    "        if len(choice_text_list) != num_choices:\n",
    "            choice_text_list = (choice_text_list + num_choices * [''])[:num_choices]\n",
    "            choice_context_list = (choice_context_list + num_choices * [None])[:num_choices]\n",
    "            if answer_id is not None and answer_id >= num_choices:\n",
    "                logging.warning(f\"Skipping question with more than {num_choices} answers: {item_json}\")\n",
    "                continue\n",
    "\n",
    "        item_ids.append(item_id)\n",
    "        questions.append(question_text)\n",
    "        choice_lists.append(choice_text_list)\n",
    "        answer_ids.append(answer_id)\n",
    "\n",
    "    data_file.close()\n",
    "    return questions, choice_lists, answer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e4d7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, questions, choices, answer_ids, tokenizer):\n",
    "        out = tokenizer(questions, max_length=25, padding=\"max_length\")\n",
    "        self.input_ids = out[\"input_ids\"]\n",
    "        self.token_type_ids = out[\"token_type_ids\"]\n",
    "        self.attention_mask = out[\"attention_mask\"]\n",
    "        self.questions = questions\n",
    "        self.choices = choices\n",
    "        self.answer_ids = answer_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[i], \n",
    "            \"attention_mask\": self.attention_mask[i], \n",
    "            \"token_type_ids\": self.token_type_ids[i],\n",
    "            \"choice_list\": self.choices[i], \n",
    "            \"answer_id\": self.answer_ids[i],\n",
    "        }\n",
    "    \n",
    "\n",
    "class RoBERTaDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, questions, choices, answer_ids, tokenizer):\n",
    "#         if \"t5\" in tokenizer.name_or_path.lower():\n",
    "#             questions = [question.replace('[MASK]', '') for question in questions]\n",
    "#         else:\n",
    "        questions = [question.replace('[MASK]', tokenizer.mask_token) for question in questions]\n",
    "        out = tokenizer(questions, max_length=25, padding=\"max_length\")\n",
    "        self.input_ids = out[\"input_ids\"]\n",
    "        self.attention_mask = out[\"attention_mask\"]\n",
    "        self.questions = questions\n",
    "        self.choices = choices\n",
    "        self.answer_ids = answer_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[i], \n",
    "            \"attention_mask\": self.attention_mask[i], \n",
    "            \"choice_list\": self.choices[i], \n",
    "            \"answer_id\": self.answer_ids[i],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f035fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, eval_dataset):\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.per_device_eval_batch_size)\n",
    "\n",
    "    logger.info(f\"***** Running evaluation  *****\")\n",
    "    logger.info(f\"  Num examples = {len(eval_dataset)}\")\n",
    "    logger.info(f\"  Batch size = {args.eval_batch_size}\")\n",
    "    eval_dataloader = tqdm(eval_dataloader, desc=\"Evaluating\")\n",
    "    \n",
    "    print(tokenizer.mask_token)\n",
    "    MASK_ID = tokenizer.encode(tokenizer.mask_token, add_special_tokens=False)\n",
    "    assert len(MASK_ID) == 1\n",
    "    MASK_ID = MASK_ID[0]\n",
    "    if \"t5\" in args.model_name_or_path.lower():\n",
    "        LABELS = tokenizer(\"<extra_id_0> blank <extra_id_1> </s>\", add_special_tokens=False, return_tensors=\"pt\")\n",
    "        LABELS = LABELS.input_ids.cuda()  # from testing, \"blank\" can be any word and results are the same \n",
    "    \n",
    "    all_answers = []\n",
    "    all_preds = []\n",
    "    for batch in eval_dataloader:\n",
    "        model.eval()\n",
    "        \n",
    "        # batch[\"choice_list\"] is [num_choices, batch_size]\n",
    "        for i in range(len(batch[\"choice_list\"][0])):\n",
    "            all_answers.append(batch[\"choice_list\"][batch[\"answer_id\"][i]][i])\n",
    "        \n",
    "        choice_lists = batch.pop(\"choice_list\")\n",
    "        del batch[\"answer_id\"] \n",
    "        for key in batch:\n",
    "            batch[key] = torch.stack(batch[key], dim=-1).cuda()\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if \"gpt\" not in args.model_name_or_path.lower():\n",
    "                if \"t5\" not in args.model_name_or_path.lower():\n",
    "                    outputs = model(**batch)\n",
    "                else:\n",
    "                    outputs = model(input_ids=batch[\"input_ids\"], labels=LABELS)\n",
    "\n",
    "                logits = outputs.logits\n",
    "                choice_ids = []\n",
    "\n",
    "                for i, logit in enumerate(logits):  # Assuming all are single tokens\n",
    "                    if \"t5\" not in args.model_name_or_path.lower():\n",
    "                        MASK_INDEX = batch[\"input_ids\"][i].tolist().index(MASK_ID)\n",
    "                    else:\n",
    "                        MASK_INDEX = 1  # TODO: not hardcode\n",
    "\n",
    "                    choice_ids = torch.tensor([tokenizer.encode(\" \" + choice_lists[j][i], add_special_tokens=False)[0] for j in range(len(choice_lists))])\n",
    "                    probs = logit[MASK_INDEX].index_select(0, choice_ids.cuda())\n",
    "                    max_ind = torch.argmax(probs)\n",
    "                    all_preds.append(choice_lists[max_ind][i])\n",
    "            else:\n",
    "                \n",
    "#         else:\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model.generate(input_ids=batch[\"input_ids\"], \n",
    "#                           num_beams=200, num_return_sequences=20,\n",
    "#                           max_length=5)\n",
    "            \n",
    "#                 match = False\n",
    "#                 for seq in outputs:\n",
    "#                     if match:\n",
    "#                         break\n",
    "#                     for i, choice in enumerate(choice_lists):\n",
    "#                         choice_id = tokenizer.encode(choice[0], add_special_tokens=False)[0]\n",
    "#                         if choice_id in seq:\n",
    "#                             all_preds.append(choice[0])\n",
    "#                             match = True\n",
    "#                             break\n",
    "\n",
    "#                 if not match:\n",
    "#                     all_preds.append(-1)\n",
    "\n",
    "    return all_answers, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4dc1b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "'''\n",
    "\"bert-base-uncased\"\n",
    "\"distilbert-base-uncased\"\n",
    "\"bert-large-uncased\"\n",
    "\"bert-large-uncased-whole-word-masking\" \n",
    "\"roberta-large\"\n",
    "\"facebook/bart-large\"\n",
    "\"t5-large\"\n",
    "\"albert-large-v1\"\n",
    "'''\n",
    "\n",
    "args.model_name_or_path = \"facebook/bart-large\"\n",
    "args.num_choices = 2\n",
    "transformers.set_seed(args.seed)\n",
    "if \"t5\" in args.model_name_or_path.lower():\n",
    "    model = transformers.T5ForConditionalGeneration.from_pretrained(args.model_name_or_path).cuda()\n",
    "    args.per_device_eval_batch_size = 1\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    tokenizer.mask_token = \"<extra_id_0>\"\n",
    "elif \"gpt\" in args.model_name_or_path.lower():\n",
    "    model = transformers.AutoModelForMaskedLM.from_pretrained(args.model_name_or_path).cuda()\n",
    "    args.per_device_eval_batch_size = 1\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    tokenizer.mask_token = \"[MASK]\"\n",
    "else:\n",
    "    model = transformers.AutoModelForMaskedLM.from_pretrained(args.model_name_or_path).cuda()\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca81746e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/31/2021 15:07:20: Reading QA instances from jsonl dataset at: data/antonym_synonym_negation_dev.jsonl\n",
      "05/31/2021 15:07:20: Sampling 200 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddbc73a667b4ae18b62a0e80178b710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/31/2021 15:07:20: Reading QA instances from jsonl dataset at: data/antonym_synonym_negation_dev.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf34948d613542d7866aa7f8f508f0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "\"data/number_comparison_age_compare_masked_dev.jsonl\"\n",
    "\"data/coffee_cats_quantifiers_dev.jsonl\"\n",
    "\"data/size_comparison_dev.jsonl\"\n",
    "\"data/antonym_synonym_negation_dev.jsonl\"\n",
    "\"data/hypernym_conjunction_dev.jsonl\"\n",
    "'''\n",
    "train_path = \"data/antonym_synonym_negation_dev.jsonl\"\n",
    "args.num_choices = 2\n",
    "eval_path = train_path #\"data/coffee_cats_quantifiers_dev.jsonl\"\n",
    "train_questions, train_choices, train_answer_ids = get_data(train_path, args.sample_train, args.num_choices)\n",
    "eval_questions, eval_choices, eval_answer_ids = get_data(eval_path, args.sample_eval, args.num_choices)\n",
    "AgeDataset = RoBERTaDataset if any(prefix in args.model_name_or_path.lower() for prefix in (\"roberta\", \"bart\", \"distil\", \"electra\", \"t5\")) else BERTDataset\n",
    "train_dataset = AgeDataset(train_questions, train_choices, train_answer_ids, tokenizer)\n",
    "eval_dataset = AgeDataset(eval_questions, eval_choices, eval_answer_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b6e8937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/31/2021 15:07:21: ***** Running evaluation  *****\n",
      "05/31/2021 15:07:21:   Num examples = 500\n",
      "05/31/2021 15:07:21:   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cefe35d9e3e4e98b7acc0672372a917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mask>\n"
     ]
    }
   ],
   "source": [
    "all_answers, all_preds = evaluate(args, model, tokenizer, eval_dataset)\n",
    "# correct = [1 if all_answers[i] == all_preds[i] else 0 for i in range(len(all_answers))]\n",
    "# print(np.array(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "06ffc47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.538"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(all_answers) == np.array(all_preds)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bebec05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23115, 254]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"smaller\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "752c9094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'er'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c1778cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "for i in range(len(all_answers)):\n",
    "    if all_preds[i] != -1:\n",
    "        b += 1\n",
    "        if all_preds[i] == all_answers[i]:\n",
    "            a += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d296355e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47345132743362833"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "900c223e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7545909849749582"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b/len(all_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ef19339",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'first_age' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-0f8db056a4a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_age\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_age\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Age Comparison {args.model_name_or_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"square\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Age 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Age 2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'first_age' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(first_age, second_age, c=all_preds, marker=\"s\")\n",
    "plt.title(f\"Age Comparison {args.model_name_or_path}\")\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"Age 1\")\n",
    "plt.ylabel(\"Age 2\")\n",
    "#plt.savefig(f\"imgs/{args.model_name_or_path.rsplit('/', 1)[-1]}-ages-double.jpg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c5549e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>The size of a nail is usually much<mask> than the size of a pen.</s><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(eval_dataset[2][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe796f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smaller'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6349e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
